# Are Large Language Models Qualified Reviewers in Originality Evaluation?

The two evaluation datasets (the Nobel evaluation dataset and the DI evaluation dataset), and our results (originality score, originality type, originality description generated by GPT-4, GPT-3.5, Mixtral-8x7b, Llama 2-70b, Llama 2-13b, Llama 2-7b) are publicly accessible for use.

## Dataset  
### (1) DI evaluation dataset
As shown in Figure 1, we build the DI evaluation dataset. According to the disruption index (DI), we also select three types of papers: *disruptive*, *developmental*, and *general*.

<p align="center">
  <img src="https://github.com/WannaLearning/Are-Large-Language-Models-Qualified-Reviewers-in-Originality-Evaluation-/blob/main/Figures-git/DI%20evaluation%20dataset.png" width="60%" alt="Topic embedding generation process"/>
</p>
<div align="center">
  Figure 1 Flowchart of DI evaluation dataset construction 
</div>  

### (2) Nobel evaluation dataset
As shown in Figure 2, we build the Nobel evaluation dataset. We selected three types of papers with different originality levels: *Nobel Prize papers*, *Nobel laureate papers*, and *random papers*. 

<p align="center">
  <img src="https://github.com/WannaLearning/Are-Large-Language-Models-Qualified-Reviewers-in-Originality-Evaluation-/blob/main/Figures-git/Nobel%20evaluation%20dataset.png" width="60%" alt="Topic embedding generation process"/>
</p>
<div align="center">
  Figure 2 Flowchart of Nobel evaluation dataset construction 
</div>  

### (3) Originality evaluation dataset
We share the originality evaluation results reported in our paper, which are generated by a range of large lanuage models (i.e.,  “GPT-3.5-Turbo-0125”, “GPT-4-0125-Preview”, "Mixtral 8x7B-Instruct", “Llama-2-7b-chat”, “Llama-2-13b-chat”, and “Llama-2-70b-chat”).   
&ensp;&ensp;&ensp;As shown in Figure 3, this study investigates LLMs’ potential as qualified reviewers in originality evaluation in zero-shot learning, utilizing a unique, manually crafted prompt. Using  biomedical papers as the data source, we constructed two evaluation datasets based on Nobel Prize papers and disruptive index. The evaluation performance of multiple LLMs of different types and scales on the datasets was scrutinized through the analysis of originality score (OS), originality type (OT), and originality description (OD), all of which were generated by the LLM. 

<p align="center">
  <img src="https://github.com/WannaLearning/Are-Large-Language-Models-Qualified-Reviewers-in-Originality-Evaluation-/blob/main/Figures-git/Research%20Framework.png" width="60%" alt="Topic embedding generation process"/>
</p>
<div align="center">
  Figure 3 Flow chart of study design
</div>  
